[

{
  "description": "SVM (Sparse, 40 features, 25 samples)",
  "code": "import osqp\nimport numpy as np\nfrom scipy import sparse\n\nn_features = 40\nn_samples = 25  \nC = 1.0\n\nX = sparse.random(n_samples, n_features, density=0.05, format='csc')\ny = 2*(np.random.rand(n_samples) > 0.5) - 1 \n\nP = sparse.block_diag([sparse.eye(n_features), \n                      sparse.csc_matrix((n_samples, n_samples))])\nq = np.hstack([np.zeros(n_features), C*np.ones(n_samples)])\n\nA = sparse.vstack([\n    sparse.hstack([sparse.diags(y) @ X, sparse.eye(n_samples)]),\n    sparse.hstack([sparse.csc_matrix((n_samples, n_features)), sparse.eye(n_samples)])\n])\nprint(f\"Total non-zeros: {A.nnz + P.nnz}\")  \n\nl = np.hstack([np.ones(n_samples), np.zeros(n_samples)])\nu = np.inf * np.ones(2 * n_samples)\n\n# Solve\nmodel = osqp.OSQP()\nmodel.setup(P, q, A, l, u, verbose=True)\nres = model.solve()\nprint(f\"SVM solved in {res.info.run_time:.2f}s, {sum(res.x[n_features:] > 1e-4)} support vectors\")"
},

    {
    "description": "21. SVM",
    "code": "import osqp\nimport numpy as np\nfrom scipy import sparse\n\n\nnp.random.seed(69)\nm, n = 50, 20  # Samples, features\nX = np.random.randn(m, n)\ny = 2 * (np.random.rand(m) > 0.5) - 1  # Binary labels\nC = 1.0\n\n# SVM QP: (1/2)||w||² + C∑ξ_i\nP = sparse.block_diag([sparse.eye(n), sparse.csc_matrix((m, m))]).tocsc()\nq = np.hstack([np.zeros(n), C * np.ones(m)])\nA = sparse.vstack([\n    sparse.hstack([sparse.diags(y) @ X, sparse.eye(m)]),  # Margin constraints\n    sparse.hstack([sparse.csc_matrix((m, n)), sparse.eye(m)])  # ξ ≥ 0\n])\nl = np.hstack([np.ones(m), np.zeros(m)])\nu = np.hstack([np.inf * np.ones(m), np.inf * np.ones(m)])\n\nprob = osqp.OSQP()\nprob.setup(P=P, q=q, A=A, l=l, u=u)\nres = prob.solve()\nprint(f\"SVM weights: {res.x[:n].round(3)}\")"
  },
{
  "description": "24. Huber",
  "code": "import osqp\nimport numpy as np\nfrom scipy import sparse\n\nn_features = 5  \nn_samples = 10 \ndelta = 1.0     \n\n# Generate tiny dataset with 2 outliers\nnp.random.seed(42)\nX = np.random.randn(n_samples, n_features)\ntrue_w = np.array([1.5, -2, 0, 3, -1])  # True coefficients\ny = X @ true_w + 0.5*np.random.randn(n_samples)\ny[[2,7]] += 8  # Add two outliers\n\n# OSQP formulation (Huber as QP)\nP = sparse.block_diag([X.T @ X,          # Quadratic term for good data\n                       sparse.eye(n_samples)])  # Linear term for outliers\nq = np.hstack([-X.T @ y,                 # Linear term\n               delta * np.ones(n_samples)])  # Huber threshold scaling\n\nA = sparse.vstack([\n    sparse.hstack([X, -sparse.eye(n_samples)]),  \n    sparse.hstack([-X, -sparse.eye(n_samples)])  \n])\nl = -np.inf * np.ones(2*n_samples)\nu = np.hstack([y, -y])\n\n# Solve\nmodel = osqp.OSQP()\nmodel.setup(P, q, A, l, u, verbose=True)\nres = model.solve()\n\n# Results\nprint(\"True coefficients:\", true_w)\nprint(\"Huber estimates:\", res.x[:n_features].round(2))\nprint(f\"Outliers detected at points: {np.where(res.x[n_features:] > delta*2)[0]}\")"
}

]